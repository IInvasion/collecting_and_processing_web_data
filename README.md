# Методы сбора и обработки данных из сети Интернет (course practice)

## Урок 1. Основы клиент-серверного взаимодействия. Парсинг API

- 1. Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.

- 2. Изучить список открытых API (https://www.programmableweb.com/category/all/apis). Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию. Ответ сервера записать в файл.

## Урок 2. Парсинг HTML. BeautifulSoup, MongoDB

Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайтов Superjob и HH. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:

* Наименование вакансии.
* Предлагаемую зарплату (отдельно минимальную, максимальную и валюту).
* Ссылку на саму вакансию.
* Сайт, откуда собрана вакансия.

По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas.

## Урок 3. Системы управления базами данных MongoDB и SQLite в Python

1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД.

2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введённой суммы.

3. Написать функцию, которая будет добавлять в вашу базу данных только новые вакансии с сайта.

## Урок 4. Парсинг HTML. XPath

1. Написать приложение, которое собирает основные новости с сайтов news.mail.ru, lenta.ru, yandex.ru/news. Для парсинга использовать XPath. Структура данных должна содержать:

* название источника;
* наименование новости;
* ссылку на новость;
* дата публикации.

2. Сложить собранные данные в БД

## Урок 5. Selenium в Python

1. Написать программу, которая собирает входящие письма из своего или тестового почтового ящика и сложить данные о письмах в базу данных (от кого, дата отправки, тема письма, текст письма полный)

2. Написать программу, которая собирает «Хиты продаж» с сайта техники mvideo и складывает данные в БД. Магазины можно выбрать свои. Главный критерий выбора: динамически загружаемые товары

## Урок 6. Scrapy

1. Создать двух пауков по сбору данных о книгах с сайтов labirint.ru и book24.ru

2. Каждый паук должен собирать:

* Ссылку на книгу
* Наименование книги
* Автор(ы)
* Основную цену
* Цену со скидкой
* Рейтинг книги

3. Собранная информация дожна складываться в базу данных

## Урок 7. Scrapy. Парсинг фото и файлов

1. Взять любую категорию товаров на сайте Леруа Мерлен. Собрать следующие данные:

* название;
* все фото;
* параметры товара в объявлении;
* ссылка;
* цена.

Не забудьте реализовать очистку и преобразование данных. Цены должны быть в виде числового значения.

2. Написать универсальный обработчик характеристик товаров, который будет формировать данные вне зависимости от их типа и количества.

3. Реализовать хранение скачиваемых файлов в отдельных папках, каждая из которых должна соответствовать собираемому товару

## Урок 8. Работа с данными

1. Написать приложение, которое будет проходиться по указанному списку двух и/или более пользователей и собирать данные об их подписчиках и подписках.

2. По каждому пользователю, который является подписчиком или на которого подписан исследуемый объект нужно извлечь имя, id, фото (остальные данные по желанию). Фото можно дополнительно скачать.

3. Собранные данные необходимо сложить в базу данных. Структуру данных нужно заранее продумать, чтобы:

4. Написать запрос к базе, который вернет список подписчиков только указанного пользователя

5. Написать запрос к базе, который вернет список профилей, на кого подписан указанный пользователь